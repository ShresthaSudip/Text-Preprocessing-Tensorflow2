{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intorduction\n",
    "Natural Language Processing (NLP) is commonly used in text classification task such as spam detection and sentiment analysis, text generation, language translations and document classification. Text data can be considered either in sequence of character, sequence of words or sequence of sentences. Most commonly, text data are considered as sequence of words for most problems. In this article we will delve into pre-processing using simple example text data. However, the material discussed here is applicable to any NLP tasks. Particularly we'll use TensorFlow2.X Keras for text pre-processing which include:\n",
    "- Tokenization\n",
    "- Sequencing\n",
    "- Padding\n",
    "\n",
    "First, let's import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer is an API available in TensorFlow Keras which is used to tokenize sentences. We have defined our text data as sentences (each separated by comma) with a python array of strings as below. There are 4 sentences including one with a maximum length of 5. Our text data also includes punctuation terms as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I want to go out.', ' I like to play.', ' No eating - ', 'No play!']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"I want to go out.\",\n",
    "             \" I like to play.\",\n",
    "             \" No eating - \",\n",
    "             \"No play!\",\n",
    "            ]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "As deep learning models do not understand text, we need to convert text into numerical representation. For this purpose, a first step is Tokenization. The Tokenizer API from TensorFlow Keras splits sentences into words and encodes these into integers. Below are hyperparameters used within Tokenizer API: \n",
    "- num_words: Limits maximum number of most popular words to keep while training. \n",
    "- filters: If not provided, by default filters out all punctuation terms (!\"#$%&()*+,-./:;<=>?@[\\]^_'{|}~\\t\\n).\n",
    "- lower=1. This is a default setting which converts all words to lower case\n",
    "- oov_tok : When its used, out of vocabulary token will be added to word index in the corpus which is used to build the model. This is used to replace out of vocabulary words (words that are not in our corpus) during text_to_sequence calls (see below).\n",
    "- word_index: Convert all words to integer index. Full list of words are available as key value property: key = word and value = token for that word\n",
    "\n",
    "Let's use the Tokenizer below and print out word index. We have used numb_words= 100 which is a lot for this data as there are only 9 distinct words and <OOV> string for out of vocabulary token.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'i': 2, 'to': 3, 'play': 4, 'no': 5, 'want': 6, 'go': 7, 'out': 8, 'like': 9, 'eating': 10}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=100, lower= 1, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, each word in our sentences has been converted to numerical tokens. For instance, \"i\" has a value of 2. The tokenizer also ignored the exclamation mark after the word. For example, there is only one token for the word \"play\" or \"play!\" i.e. 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequencing\n",
    "\n",
    "Next, let's represent each sentence by sequences of numbers using texts_to_sequences from tokenizer object. Below, we printed out raw sentences, word index and sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I want to go out.', ' I like to play.', ' No eating - ', 'No play!']\n",
      "{'<OOV>': 1, 'i': 2, 'to': 3, 'play': 4, 'no': 5, 'want': 6, 'go': 7, 'out': 8, 'like': 9, 'eating': 10}\n",
      "[[2, 6, 3, 7, 8], [2, 9, 3, 4], [5, 10], [5, 4]]\n"
     ]
    }
   ],
   "source": [
    "# Text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sentences)\n",
    "print(word_index)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, texts are converted to sequences. \n",
    "- List of sentences have been converted to integers. For example, \n",
    "- \"I want to go out\" ---> [2,6,3,7,8]\n",
    "- \"I like to play\" ---> [2,9,3,4]\n",
    "- \"No eating\" ---> [5,10]\n",
    "- \"No play!\" ---> [5,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "In any raw text data, naturally there will be sentences of different lengths. However, all neural networks require to have the inputs with same size. For this purpose, padding is done. Below, let's use pad_sequences for padding. pad_sequences uses arguments such as sequences, padding, maxlen, truncating, value and dtype.\n",
    "- sequences : list of sequences that we created earlier\n",
    "- padding = 'pre' or 'post (default pre). By using pre, we'll pad before each sequence and post will pad after each sequence.\n",
    "- maxlen = maximum length of all sequences. If not provided, by default it will use the maximum length of the longest sentence.\n",
    "- truncating = 'pre' or 'post' (default 'pre'). If a sequence length is larger than the provided maxlen value then, these values will be truncated to maxlen. 'pre' option will truncate at the beginning where as 'post' will truncate at the end of the sequences.\n",
    "- value: padding value (default is 0)\n",
    "- dtype : output sequence type (default is int32)\n",
    "\n",
    "Let's focus important arguments used in pad_sequences : padding, maxlen and truncating.\n",
    "\n",
    "##### pre  and post padding\n",
    "\n",
    "Use of 'pre' or 'post' padding depends upon the analysis. In some cases, padding at the beginning is appropriate while not in others. For instance, if we use Recurrent Neural Network (RNN) for spam detection, then padding at the beginning would be appropriate as RNN can not learn long distance patterns. Padding at the beginning allows us to keep the sequences in the end hence RNN can make use of these sequences for prediction of next. However, in any case padding should be conducted after careful consideration and business knowledge. \n",
    "\n",
    "Below, the outputs for 'pre' followed by 'post' padding are shown with default maxlen value of maximum length of sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word_index =  {'<OOV>': 1, 'i': 2, 'to': 3, 'play': 4, 'no': 5, 'want': 6, 'go': 7, 'out': 8, 'like': 9, 'eating': 10}\n",
      "\n",
      "sequences =  [[2, 6, 3, 7, 8], [2, 9, 3, 4], [5, 10], [5, 4]]\n",
      "\n",
      "padded_seq = \n",
      "[[ 2  6  3  7  8]\n",
      " [ 0  2  9  3  4]\n",
      " [ 0  0  0  5 10]\n",
      " [ 0  0  0  5  4]]\n"
     ]
    }
   ],
   "source": [
    "# pre paddding\n",
    "pre_pad = pad_sequences(sequences, padding='pre')\n",
    "print(\"\\nword_index = \", word_index)\n",
    "print(\"\\nsequences = \", sequences)\n",
    "print(\"\\npadded_seq = \" )\n",
    "print(pre_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example above, the sequence with maximum length is [ 2, 6, 3, 7, 8] which corresponds to \"I want to go out\". When padding ='pre' is used, padded value of 0 is added at the beginning of all other sequences. Because other sequences have shorter sequence than [ 2, 6, 3, 7, 8], padding actually made all other sequences to be of same size with this sequence.\n",
    "\n",
    "Whereas, when padding = 'post' is used , padded value i.e. 0 is added at the end of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word_index =  {'<OOV>': 1, 'i': 2, 'to': 3, 'play': 4, 'no': 5, 'want': 6, 'go': 7, 'out': 8, 'like': 9, 'eating': 10}\n",
      "\n",
      "sequences =  [[2, 6, 3, 7, 8], [2, 9, 3, 4], [5, 10], [5, 4]]\n",
      "\n",
      "padded_seq = \n",
      "[[ 2  6  3  7  8]\n",
      " [ 2  9  3  4  0]\n",
      " [ 5 10  0  0  0]\n",
      " [ 5  4  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# post paddding\n",
    "post_pad = pad_sequences(sequences, padding='post')\n",
    "print(\"\\nword_index = \", word_index)\n",
    "print(\"\\nsequences = \", sequences)\n",
    "print(\"\\npadded_seq = \" )\n",
    "print(post_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pre  and post padding with maxlen and truncating option\n",
    "By use of maxlen =4, we are truncating the length of padded sequences to be less than or equal to 4. As shown, below, the use of maxlen=4 impacted the first sequence [2, 6, 3, 7, 8]. THis sequence has length of 5 and is truncated to 4. The truncating with 'pre' option allows us to truncate the sequence at the beginning. Whereas, truncating with 'post' will truncate the sequence at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  3  7  8]\n",
      " [ 2  9  3  4]\n",
      " [ 0  0  5 10]\n",
      " [ 0  0  5  4]]\n"
     ]
    }
   ],
   "source": [
    "# pre padding, maxlen and pre truncation\n",
    "prepad_maxlen_pretrunc = pad_sequences(sequences, padding = 'pre', maxlen =4, truncating = 'pre')\n",
    "print(prepad_maxlen_pretrunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  6  3  7]\n",
      " [ 2  9  3  4]\n",
      " [ 0  0  5 10]\n",
      " [ 0  0  5  4]]\n"
     ]
    }
   ],
   "source": [
    "# pre padding, maxlen and post truncation\n",
    "prepad_maxlen_posttrunc = pad_sequences(sequences, padding = 'pre', maxlen =4, truncating = 'post')\n",
    "print(prepad_maxlen_posttrunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  3  7  8]\n",
      " [ 2  9  3  4]\n",
      " [ 5 10  0  0]\n",
      " [ 5  4  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# post padding, maxlen and pre truncation\n",
    "postpad_maxlen_pretrunc = pad_sequences(sequences, padding = 'post', maxlen =4, truncating = 'pre')\n",
    "print(postpad_maxlen_pretrunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  6  3  7]\n",
      " [ 2  9  3  4]\n",
      " [ 5 10  0  0]\n",
      " [ 5  4  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# post padding, maxlen and post truncation\n",
    "postpad_maxlen_pretrunc = pad_sequences(sequences, padding = 'post', maxlen =4, truncating = 'post')\n",
    "print(postpad_maxlen_pretrunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article, we focused on pre-processing raw text data and preparing it for deep learning models. Specifically, we covered tokenizing sentences, representing it as sequences and padding it to make all the sequences of same length. This padded sequences are now ready for train/test split that can be used for neural network. Please refer to  [video by Laurence Moroney's](https://www.youtube.com/watch?v=fNxaJsNG3-s)\n",
    "NLP zero to hero for further reading.\n",
    "\n",
    "In future article, I will explain how we can use pre-processing in a real world data and use such padded sequence data with embedding in deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
